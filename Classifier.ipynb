{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score, train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = './EEGData/MNE-eegbci-data/files/eegmmidb/1.0.0'\n",
    "subdirectories = [f.path for f in os.scandir(main_folder) if f.is_dir()]\n",
    "opened_files = []\n",
    "closed_files = []\n",
    "\n",
    "for subdirectory in subdirectories:\n",
    "    files = os.listdir(subdirectory)\n",
    "    if len(files) > 0:\n",
    "        for file in files:\n",
    "            if file[-6:] == '01.edf':\n",
    "                # This is data for eyes opened\n",
    "                eyes_opened = os.path.join(subdirectory, file)\n",
    "                opened_files.append(eyes_opened)\n",
    "            if file[-6:] == '02.edf':\n",
    "                # This is data for eyes closed\n",
    "                eyes_closed = os.path.join(subdirectory, file)\n",
    "                closed_files.append(eyes_closed)\n",
    "    else:\n",
    "        print(f\"No files found in {subdirectory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_open_data_raw = []\n",
    "large_closed_data_row = []\n",
    "\n",
    "for data in opened_files:\n",
    "    large_open_data_raw.append(mne.io.read_raw_edf(data, preload=True, verbose=False).get_data(verbose=False))\n",
    "\n",
    "for data in closed_files:\n",
    "    large_closed_data_row.append(mne.io.read_raw_edf(data, preload=True, verbose=False).get_data(verbose=False))\n",
    "\n",
    "print(large_open_data_raw.__len__())\n",
    "print(large_closed_data_row.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_channel_plot(data, channel, title):\n",
    "    plt.figure(figsize=(20, 3))\n",
    "    plt.plot(data[channel, :])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "single_channel_plot(large_open_data_raw[0], 0, \"Channel 0 for eyes opened\")\n",
    "single_channel_plot(large_closed_data_row[0], 0, \"Channel 0 for eyes closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_channels(data, title):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(data.T)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_all_channels(large_open_data_raw[0], \"All channels for eyes opened\")\n",
    "plot_all_channels(large_closed_data_row[0], \"All channels for eyes closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_channels_subplots(data):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(64):\n",
    "        plt.subplot(8, 8, i+1)\n",
    "        plt.plot(data[i, :])\n",
    "        plt.title(f\"Channel {i}\")\n",
    "    plt.show() \n",
    "\n",
    "plot_all_channels_subplots(large_open_data_raw[0])\n",
    "plot_all_channels_subplots(large_closed_data_row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_closed_data = []\n",
    "large_open_data = []\n",
    "\n",
    "data1 = mne.io.read_raw_edf(data, preload=True, verbose=False)\n",
    "\n",
    "# Define the frequency range for the filter\n",
    "low_freq = 0.1  # Low-pass frequency in Hz\n",
    "high_freq = 35 # High-pass frequency in Hz\n",
    "\n",
    "for data in opened_files[:10]:\n",
    "    data1 = mne.io.read_raw_edf(data, preload=True, verbose=False)\n",
    "    data1 = mne.filter.filter_data(data1.get_data(), sfreq=data1.info['sfreq'], l_freq=low_freq, h_freq=high_freq, fir_design=\"firwin\", verbose=False)\n",
    "    eeg_data1 = data1\n",
    "    large_open_data.append(eeg_data1)\n",
    "\n",
    "for data in closed_files[:10]:\n",
    "    data1 = mne.io.read_raw_edf(data, preload=True, verbose=False)\n",
    "    data1 = mne.filter.filter_data(data1.get_data(), sfreq=data1.info['sfreq'], l_freq=low_freq, h_freq=high_freq, fir_design=\"firwin\", verbose=False)\n",
    "    eeg_data1 = data1\n",
    "    large_closed_data.append(eeg_data1)\n",
    "\n",
    "plot_all_channels(large_open_data[0], \"All channels for eyes opened\")\n",
    "plot_all_channels(large_closed_data[0], \"All channels for eyes closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_channels_subplots(large_open_data_raw[0])\n",
    "plot_all_channels_subplots(large_open_data[0])\n",
    "\n",
    "plot_all_channels_subplots(large_closed_data_row[0])\n",
    "plot_all_channels_subplots(large_closed_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Channel Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 0\n",
    "data = np.concatenate((np.array(large_closed_data)[:, channel, :], np.array(large_open_data)[:, channel, :]), axis=0)\n",
    "labels = np.concatenate((np.zeros(len(np.array(large_closed_data)[:, channel, :])), np.ones(len(np.array(large_open_data)[:, channel, :]))), axis=0)\n",
    "data = torch.Tensor(data).to(device)\n",
    "labels = torch.Tensor(labels).to(device)\n",
    "labels = labels.long()\n",
    "\n",
    "random_indices = np.arange(len(data))\n",
    "np.random.shuffle(random_indices)\n",
    "data = data[random_indices]\n",
    "labels = labels[random_indices]\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "test_size = 0.2 \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "train_data = torch.Tensor(train_data)\n",
    "train_labels = torch.Tensor(train_labels)\n",
    "test_data = torch.Tensor(test_data)\n",
    "test_labels = torch.Tensor(test_labels)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNClassifierSingle(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FNNClassifierSingle, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_size = 9760\n",
    "hidden_size = 128\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_model = FNNClassifierSingle(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fnn_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = fnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader: \n",
    "        outputs = fnn_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test dataset: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Channel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((np.array(large_closed_data), np.array(large_open_data)), axis=0)\n",
    "labels = np.concatenate((np.zeros(len(np.array(large_closed_data))), np.ones(len(np.array(large_open_data)))), axis=0)\n",
    "data = torch.Tensor(data)\n",
    "labels = torch.Tensor(labels)\n",
    "labels = labels.long()\n",
    "\n",
    "random_indices = np.arange(len(data))\n",
    "np.random.shuffle(random_indices)\n",
    "data = data[random_indices]\n",
    "labels = labels[random_indices]\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "test_size = 0.2 \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "train_data = torch.Tensor(train_data)\n",
    "train_labels = torch.Tensor(train_labels)\n",
    "test_data = torch.Tensor(test_data)\n",
    "test_labels = torch.Tensor(test_labels)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FNNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_size = 9760  * 64\n",
    "hidden_size = 128\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_model = FNNClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fnn_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = fnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}] Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader: \n",
    "        outputs = fnn_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test dataset: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
